{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ebfed17-ded4-4b32-8fc8-862fb77b252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-09 06:59:09 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 06:59:10,668\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "import re\n",
    "from vllm import SamplingParams\n",
    "\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f7e6d6-0a10-40dd-b82d-bd123d0886cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.4: Fast Qwen2 patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA A10G. Max memory: 22.184 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/DeepSeek-R1-Distill-Qwen-1.5B with actual GPU utilization = 49.44%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 22.18 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32000. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 7.52 GB. Also swap space = 6 GB.\n",
      "INFO 02-09 06:59:28 config.py:542] This model supports multiple tasks: {'reward', 'generate', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 02-09 06:59:28 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='unsloth/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "INFO 02-09 06:59:29 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-09 06:59:29 model_runner.py:1110] Starting to load model unsloth/DeepSeek-R1-Distill-Qwen-1.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W209 06:59:29.013430000 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-09 06:59:30 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 02-09 06:59:30 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-09 06:59:30 model_runner.py:1115] Loading model weights took 3.3470 GB\n",
      "INFO 02-09 06:59:31 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-09 06:59:34 worker.py:267] Memory profiling takes 3.13 seconds\n",
      "INFO 02-09 06:59:34 worker.py:267] the current vLLM instance can use total_gpu_memory (22.18GiB) x gpu_memory_utilization (0.49) = 10.97GiB\n",
      "INFO 02-09 06:59:34 worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 5.54GiB.\n",
      "INFO 02-09 06:59:34 executor_base.py:110] # CUDA blocks: 12967, # CPU blocks: 14043\n",
      "INFO 02-09 06:59:34 executor_base.py:115] Maximum concurrency for 32000 tokens per request: 6.48x\n",
      "INFO 02-09 06:59:38 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:15<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-09 06:59:54 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.27 GiB\n",
      "INFO 02-09 06:59:54 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 23.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.2.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 32000 # Can increase for longer reasoning traces\n",
    "lora_rank = 64 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.95, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ff2f65-bf68-45f0-a26c-c1609d9e6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract content from within \\boxed{} notation\n",
    "    Args:\n",
    "        text: Text containing LaTeX boxed content\n",
    "    Returns:\n",
    "        Content within boxed notation or empty string if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        answer = text.split(\"</think>\")[-1]\n",
    "        # Look for content between \\boxed{...}\n",
    "        boxed = re.search(r\"\\\\boxed{(.*?)}\", answer)\n",
    "        return boxed.group(1) if boxed else \"\"\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36d85f54-eac8-4158-bb62-52c5427afc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.6,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 32000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded06f8c",
   "metadata": {},
   "source": [
    "# inference without lora and with lora trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17676d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Ahmed and Emily are having a contest to see who can get the best grade in the class. There have been 9 assignments and Ahmed has a 91 in the class. Emily has a 92. The final assignment is worth the same amount as all the other assignments. Emily got a 90 on the final assignment. What is the minimum grade Ahmed needs to get to beat Emily if all grades are whole numbers?\n",
    "Please reason step by step, and put your final answer within \\boxed{}\"\"\"\n",
    "\n",
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : question},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : question},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "\n",
    "lora_path = \"\" # path to the LoRA checkpoint\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(lora_path),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
